{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python310\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pre-Processing In NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Steps for Text Preprocessing**\n",
    "- Tokenization\n",
    "- Part of speech(POS) tagging\n",
    "- Stop words removal \n",
    "- Stremming\n",
    "- Lemmatization\n",
    "- Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Tokenization**\n",
    "- Tokenization is the process of breaking up text into the smaller parts called tokens.\n",
    "- A token may be a word, part of a word, or just characters like punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **White Space Tokenization**\n",
    "- The simplest way to tokenize text is to use whitespace within a string as the \"delimiter\" of words. \n",
    "- This can be accomplished with python's split function, which is available on all string object instances as well as on the string built-in class itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'was', 'born', 'in', 'Ahmedabad', 'in', '2000']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"I was born in Ahmedabad in 2000\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was born in Ahmedabad in 2000 ', ' and i am 24 years old']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"I was born in Ahmedabad in 2000 , and i am 24 years old\"\n",
    "text.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLTK Word Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import(sent_tokenize,word_tokenize,TreebankWordTokenizer,wordpunct_tokenize,TweetTokenizer,MWETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Natural Language processing is an amazing topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Natural', 'Language', 'processing', 'is', 'an', 'amazing', 'topic', '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language processing is an amazing topic.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wordpunct_tokenize** tokenizer splits the sentences into words based on whitespaces and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hope',\n",
       " 'is',\n",
       " 'the',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'stronger',\n",
       " 'than',\n",
       " 'fear',\n",
       " '#',\n",
       " 'HOPE',\n",
       " '#',\n",
       " 'STRONG']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Hope is the only thing stronger than fear # HOPE # STRONG\"\n",
    "wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treebank word tokenizer** incorporates a variety of common rules for english word tokenization. it separates pharase-terminating punctuation like (?!.;,) from adjacent tokens & retains decimal numbers as a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'yourself',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'do',\n",
       " 'with',\n",
       " 'others',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"What you don't want to do with yourself don't do with others.\"\n",
    "tokanizer=TreebankWordTokenizer()\n",
    "tokanizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweet tokenizer** when we want to apply tokenization in text data like tweets, the tweet tokenizer can produce practical tokens.\n",
    "- Through this issue, NLTK has a rule based tokenizer special for tweets.\n",
    "- We can split emojis into different words if we need them for tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Just',\n",
       " 'learned',\n",
       " 'about',\n",
       " '#NLTK',\n",
       " 'and',\n",
       " 'its',\n",
       " 'TweetTokenizer',\n",
       " '!',\n",
       " '\\U0001f979',\n",
       " '#NLP']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=\"Just learned about #NLTK and its TweetTokenizer! ðŸ¥¹ #NLP\"\n",
    "tokenizer=TweetTokenizer()\n",
    "tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **MWE Tokenizer** can merge multi-word expressions into single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'natural_language_Toolkit',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'tool',\n",
       " 'for',\n",
       " 'NLP_tasks',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"The natural language Toolkit is a great tool for NLP tasks.\"\n",
    "MWE_TOKENIZER=MWETokenizer([('natural','language','Toolkit'),('NLP','tasks')])\n",
    "tokens=MWE_TOKENIZER.tokenize(nltk.word_tokenize(text))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let us tokenize the same text using regular expression tokenizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8733942263']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer=RegexpTokenizer(pattern='\\d+$')\n",
    "tokens=reg_tokenizer.tokenize(\"My contact num is 8733942263\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bigrams, Trigrams & Ngrams**\n",
    "- Bigrams- tokens of two consecutive written words are known as Bigrams\n",
    "- Trigrams- tokens of three consecutive written words are known as Trigrams\n",
    "- Ngrams- tokens of any number of  consecutive written words are known as Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"ML can be seen as a time-saving device that allows humans to explore their more creative ambitions while ML is in the background crunching numbers\"\n",
    "ml_tokens=nltk.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ML', 'can'),\n",
       " ('can', 'be'),\n",
       " ('be', 'seen'),\n",
       " ('seen', 'as'),\n",
       " ('as', 'a'),\n",
       " ('a', 'time-saving'),\n",
       " ('time-saving', 'device'),\n",
       " ('device', 'that'),\n",
       " ('that', 'allows'),\n",
       " ('allows', 'humans'),\n",
       " ('humans', 'to'),\n",
       " ('to', 'explore'),\n",
       " ('explore', 'their'),\n",
       " ('their', 'more'),\n",
       " ('more', 'creative'),\n",
       " ('creative', 'ambitions'),\n",
       " ('ambitions', 'while'),\n",
       " ('while', 'ML'),\n",
       " ('ML', 'is'),\n",
       " ('is', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'background'),\n",
       " ('background', 'crunching'),\n",
       " ('crunching', 'numbers')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_bigrams=list(nltk.bigrams(ml_tokens))\n",
    "ml_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ML', 'can', 'be'),\n",
       " ('can', 'be', 'seen'),\n",
       " ('be', 'seen', 'as'),\n",
       " ('seen', 'as', 'a'),\n",
       " ('as', 'a', 'time-saving'),\n",
       " ('a', 'time-saving', 'device'),\n",
       " ('time-saving', 'device', 'that'),\n",
       " ('device', 'that', 'allows'),\n",
       " ('that', 'allows', 'humans'),\n",
       " ('allows', 'humans', 'to'),\n",
       " ('humans', 'to', 'explore'),\n",
       " ('to', 'explore', 'their'),\n",
       " ('explore', 'their', 'more'),\n",
       " ('their', 'more', 'creative'),\n",
       " ('more', 'creative', 'ambitions'),\n",
       " ('creative', 'ambitions', 'while'),\n",
       " ('ambitions', 'while', 'ML'),\n",
       " ('while', 'ML', 'is'),\n",
       " ('ML', 'is', 'in'),\n",
       " ('is', 'in', 'the'),\n",
       " ('in', 'the', 'background'),\n",
       " ('the', 'background', 'crunching'),\n",
       " ('background', 'crunching', 'numbers')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_trigrams=list(nltk.trigrams(ml_tokens))\n",
    "ml_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS Tagging (Parts of Speech)**\n",
    "1. once the tokens are generated, the next step is to tag the tokens with  respect to their POS\n",
    "2. POS Tagging is the process of labeling tokens with respective parts of speech.\n",
    "3. These tags take into "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger_eng: <urlopen\n",
      "[nltk_data]     error [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ML', 'NNP'), ('can', 'MD'), ('solve', 'VB'), ('problems', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('solved', 'VBN'), ('by', 'IN'), ('numerical', 'JJ'), ('means', 'NNS'), ('alone', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the text\n",
    "text = \"ML can solve problems that cannot be solved by numerical means alone\"\n",
    "\n",
    "# Tokenize and apply POS tagging\n",
    "pos_tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jin', 'NN'), ('eats', 'VBZ'), ('a', 'DT'), ('bananas', 'NN')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"jin eats a bananas\"\n",
    "nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stop Words Removal - 28-01-2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  The quick brown fox jumped over the lazy dog\n",
      "Text after stopword removal:  quick brown fox jumped lazy dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Another sample text\n",
    "new_text=\"The quick brown fox jumped over the lazy dog\"\n",
    "\n",
    "# Tokenize the new text using NLTK\n",
    "new_word=word_tokenize(new_text)\n",
    "\n",
    "# Remove stop words using NLTK\n",
    "new_filtered_words= [word for word in new_word if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# Join the filtered words to form a clean text\n",
    "clean_text=\" \".join(new_filtered_words)\n",
    "print(\"Original text: \" , new_text)\n",
    "print(\"Text after stopword removal: \" , clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\python310\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\python310\\lib\\site-packages (from gensim) (1.25.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\python310\\lib\\site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python310\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  The majestic mountains provide a breathtaking view\n",
      "Text after stopword removal:  The majestic mountains provide breathtaking view\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Another sample text\n",
    "new_text=\"The majestic mountains provide a breathtaking view\"\n",
    "\n",
    "# Remove stop words using gensim\n",
    "new_filtered_words=remove_stopwords(new_text)\n",
    "\n",
    "print(\"Original text: \" , new_text)\n",
    "print(\"Text after stopword removal: \" , new_filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stemming - 30-01-2025**\n",
    "Steming is converting words into base forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "# Stemming using NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "print(pst.stem(\"Change\"))\n",
    "print(pst.stem(\"Changing\"))\n",
    "print(pst.stem(\"changes\"))\n",
    "print(pst.stem(\"Changed\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NonEnglish Stemmers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst=SnowballStemmer(\"spanish\")\n",
    "print(sbst.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produccion\n",
      "product\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst=SnowballStemmer(\"spanish\")\n",
    "print(sbst.stem(\"produccion\"))\n",
    "print(sbst.stem(\"producto\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lemmatization - 31-01-2025**\n",
    "- A way of converting inflected form of a root word using parts of speech & context as a base\n",
    "- It applies different rules to each POS to get the root word called lemma\n",
    "- The root thus obtained has a grammatical meaning unlike stemming\n",
    "- The additinal feature makes the process slower as compared to stemming.\n",
    "- for example , a lemmatizer should map gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\divu2\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem=WordNetLemmatizer()\n",
    "print(word_lem.lemmatize(\"eating\", pos=\"v\"))\n",
    "print(word_lem.lemmatize(\"eats\", pos=\"v\"))\n",
    "print(word_lem.lemmatize(\"ate\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing Lemmatization with Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of WordNetLemmatizer go\n",
      "\n",
      "Result of PorterStemmer: gone\n",
      "\n",
      "Result of LancasterStemmer: gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import*\n",
    "print(\"Result of WordNetLemmatizer\",\n",
    "word_lem.lemmatize(\"gone\",pos=\"v\"))\n",
    "\n",
    "print(\"\\nResult of PorterStemmer:\",\n",
    "      PorterStemmer().stem(\"gone\"))\n",
    "\n",
    "print(\"\\nResult of LancasterStemmer:\",\n",
    "      LancasterStemmer().stem(\"gone\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemmatization generates the correct morphological root of the word using POS key, Porter Stemmer does not stem the word at all wheares lancaster stemmer performs over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
