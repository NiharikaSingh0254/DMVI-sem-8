{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1. Problem Statement: Feature Extraction**\n",
    "\n",
    "Write a Python program that reads the demotext2.txt text file.\n",
    "\n",
    "The following are the tasks that are to be taken into consideration while constructing the solution\n",
    "\n",
    "1. Load the demotext2.txt text file into a variable and then close the file\n",
    "\n",
    "2. Do sentence wise tokenization and list out generated tokens\n",
    "\n",
    "3. Transform each token into a lower case\n",
    "\n",
    "4. Do vectorization using TFID Vectorizer\n",
    "\n",
    "5. Generate vector-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\divu2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1 import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2 Load the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He can even spout some sports trivia and Christmas carols and stuff like that.\n",
      "We'd talk sports and stuff, and maybe have a beer.\n",
      "The Admirable Crichton of his day, he was keen alike on field sports and the arts, the friend and admirer equally of Cecil Rhodes and of Rodin, a railway director and a yeomanry colonel.\n",
      "But he was not brought forward by his father or prepared in any way for his future greatness, and lived in the country occupied with field sports, till after the institution of the second protectorate in 16J7 and the recognition of Oliver's right to name his successor.\n",
      "Thankfully, he was too concerned with sports to get in any real trouble.\n",
      "See Strutt, Sports and Pastimes, who also gives an illustration, \"taken from a manuscriptal painting of the 9th century in the Cotton Library,\" representing \"a Saxon chieftain, attended by his huntsman and a couple of hounds, pursuing the wild swine in a forest.\"\n",
      "As they entered the yard, Carmen noticed Lori's little red sports car.\n",
      "After a lengthy shower, Jenn exited and pulled on clean leggings, sports bra, and socks.\n",
      "A park and sports ground at the western end of the town contains the pedestal for a statue of President Kruger.\n",
      "There were crude medieval notions that fossils were \" freaks \" or \" sports \" of nature (lusus naturae), or that they represented failures of a creative force within the earth (a notion of Greek and Arabic origin), or that larger and smaller fossils represented the remains of races of giants or of pygmies (the mythical idea).\n"
     ]
    }
   ],
   "source": [
    "f = open(\"demotext2.txt\", \"r\")\n",
    "#read whole file to a string\n",
    "text = f.read()\n",
    " \n",
    "#close file\n",
    "f.close()\n",
    " \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3 Tokenize the sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He can even spout some sports trivia and Christmas carols and stuff like that.', \"We'd talk sports and stuff, and maybe have a beer.\", 'The Admirable Crichton of his day, he was keen alike on field sports and the arts, the friend and admirer equally of Cecil Rhodes and of Rodin, a railway director and a yeomanry colonel.', \"But he was not brought forward by his father or prepared in any way for his future greatness, and lived in the country occupied with field sports, till after the institution of the second protectorate in 16J7 and the recognition of Oliver's right to name his successor.\", 'Thankfully, he was too concerned with sports to get in any real trouble.', 'See Strutt, Sports and Pastimes, who also gives an illustration, \"taken from a manuscriptal painting of the 9th century in the Cotton Library,\" representing \"a Saxon chieftain, attended by his huntsman and a couple of hounds, pursuing the wild swine in a forest.\"', \"As they entered the yard, Carmen noticed Lori's little red sports car.\", 'After a lengthy shower, Jenn exited and pulled on clean leggings, sports bra, and socks.', 'A park and sports ground at the western end of the town contains the pedestal for a statue of President Kruger.', 'There were crude medieval notions that fossils were \" freaks \" or \" sports \" of nature (lusus naturae), or that they represented failures of a creative force within the earth (a notion of Greek and Arabic origin), or that larger and smaller fossils represented the remains of races of giants or of pygmies (the mythical idea).']\n"
     ]
    }
   ],
   "source": [
    "nltk_token=nltk.sent_tokenize(text)\n",
    "print(nltk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4  Converting  all the words in the sentences to lower case for generating tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a park and sports ground at the western end of the town contains the pedestal for a statue of president kruger.',\n",
       " \"as they entered the yard, carmen noticed lori's little red sports car.\",\n",
       " 'there were crude medieval notions that fossils were \" freaks \" or \" sports \" of nature (lusus naturae), or that they represented failures of a creative force within the earth (a notion of greek and arabic origin), or that larger and smaller fossils represented the remains of races of giants or of pygmies (the mythical idea).',\n",
       " 'the admirable crichton of his day, he was keen alike on field sports and the arts, the friend and admirer equally of cecil rhodes and of rodin, a railway director and a yeomanry colonel.',\n",
       " \"but he was not brought forward by his father or prepared in any way for his future greatness, and lived in the country occupied with field sports, till after the institution of the second protectorate in 16j7 and the recognition of oliver's right to name his successor.\",\n",
       " 'thankfully, he was too concerned with sports to get in any real trouble.',\n",
       " 'after a lengthy shower, jenn exited and pulled on clean leggings, sports bra, and socks.',\n",
       " \"we'd talk sports and stuff, and maybe have a beer.\",\n",
       " 'see strutt, sports and pastimes, who also gives an illustration, \"taken from a manuscriptal painting of the 9th century in the cotton library,\" representing \"a saxon chieftain, attended by his huntsman and a couple of hounds, pursuing the wild swine in a forest.\"',\n",
       " 'he can even spout some sports trivia and christmas carols and stuff like that.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Setting all the sentance  to lower case\n",
    "nltk_tokens = set(w.lower() for w in nltk_token)\n",
    "nltk_tokens = list(nltk_tokens)\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step-5: Vectorizing sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 57)\t0.3140883677709186\n",
      "  (0, 84)\t0.3140883677709186\n",
      "  (0, 107)\t0.3140883677709186\n",
      "  (0, 82)\t0.3140883677709186\n",
      "  (0, 21)\t0.3140883677709186\n",
      "  (0, 116)\t0.3140883677709186\n",
      "  (0, 31)\t0.3140883677709186\n",
      "  (0, 120)\t0.3140883677709186\n",
      "  (0, 49)\t0.3140883677709186\n",
      "  (0, 105)\t0.11612481349824408\n",
      "  (0, 80)\t0.3140883677709186\n",
      "  (1, 11)\t0.35057103775126797\n",
      "  (1, 93)\t0.35057103775126797\n",
      "  (1, 63)\t0.35057103775126797\n",
      "  (1, 65)\t0.35057103775126797\n",
      "  (1, 73)\t0.35057103775126797\n",
      "  (1, 12)\t0.35057103775126797\n",
      "  (1, 122)\t0.35057103775126797\n",
      "  (1, 32)\t0.35057103775126797\n",
      "  (1, 105)\t0.12961319346421593\n",
      "  (2, 52)\t0.17921062626016357\n",
      "  (2, 70)\t0.17921062626016357\n",
      "  (2, 88)\t0.17921062626016357\n",
      "  (2, 45)\t0.17921062626016357\n",
      "  (2, 89)\t0.17921062626016357\n",
      "  :\t:\n",
      "  (8, 24)\t0.21254144490425056\n",
      "  (8, 51)\t0.21254144490425056\n",
      "  (8, 7)\t0.21254144490425056\n",
      "  (8, 16)\t0.21254144490425056\n",
      "  (8, 100)\t0.21254144490425056\n",
      "  (8, 96)\t0.21254144490425056\n",
      "  (8, 61)\t0.21254144490425056\n",
      "  (8, 22)\t0.21254144490425056\n",
      "  (8, 15)\t0.21254144490425056\n",
      "  (8, 1)\t0.21254144490425056\n",
      "  (8, 79)\t0.21254144490425056\n",
      "  (8, 67)\t0.21254144490425056\n",
      "  (8, 112)\t0.21254144490425056\n",
      "  (8, 53)\t0.21254144490425056\n",
      "  (8, 46)\t0.21254144490425056\n",
      "  (8, 81)\t0.21254144490425056\n",
      "  (8, 108)\t0.21254144490425056\n",
      "  (8, 105)\t0.07858086507729196\n",
      "  (9, 62)\t0.41311916325859716\n",
      "  (9, 13)\t0.41311916325859716\n",
      "  (9, 17)\t0.41311916325859716\n",
      "  (9, 117)\t0.41311916325859716\n",
      "  (9, 106)\t0.41311916325859716\n",
      "  (9, 109)\t0.3511890313276876\n",
      "  (9, 105)\t0.15273849880663135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vect = TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')\n",
    "tf_matrix = tf_vect.fit_transform(nltk_tokens)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6 Vector matrix formation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16j7</th>\n",
       "      <th>9th</th>\n",
       "      <th>admirable</th>\n",
       "      <th>admirer</th>\n",
       "      <th>alike</th>\n",
       "      <th>arabic</th>\n",
       "      <th>arts</th>\n",
       "      <th>attended</th>\n",
       "      <th>beer</th>\n",
       "      <th>bra</th>\n",
       "      <th>...</th>\n",
       "      <th>thankfully</th>\n",
       "      <th>till</th>\n",
       "      <th>town</th>\n",
       "      <th>trivia</th>\n",
       "      <th>trouble</th>\n",
       "      <th>way</th>\n",
       "      <th>western</th>\n",
       "      <th>wild</th>\n",
       "      <th>yard</th>\n",
       "      <th>yeomanry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.224397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.224397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.224397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.49167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413119</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       16j7       9th  admirable   admirer     alike    arabic      arts  \\\n",
       "0  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000   0.000000  0.000000  0.000000  0.179211  0.000000   \n",
       "3  0.000000  0.000000   0.243545  0.243545  0.243545  0.000000  0.243545   \n",
       "4  0.224397  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.212541   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   attended     beer      bra  ...  thankfully      till      town    trivia  \\\n",
       "0  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.314088  0.000000   \n",
       "1  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.00000  0.00000  ...     0.00000  0.224397  0.000000  0.000000   \n",
       "5  0.000000  0.00000  0.00000  ...     0.49167  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.00000  0.33083  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.50903  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "8  0.212541  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.413119   \n",
       "\n",
       "   trouble       way   western      wild      yard  yeomanry  \n",
       "0  0.00000  0.000000  0.314088  0.000000  0.000000  0.000000  \n",
       "1  0.00000  0.000000  0.000000  0.000000  0.350571  0.000000  \n",
       "2  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.00000  0.000000  0.000000  0.000000  0.000000  0.243545  \n",
       "4  0.00000  0.224397  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.49167  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8  0.00000  0.000000  0.000000  0.212541  0.000000  0.000000  \n",
       "9  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 124 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tf_names = tf_vect.get_feature_names_out()\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(), columns=tf_names)\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The sentences in the dataset has been changed into word tokens and then later converted to Vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
